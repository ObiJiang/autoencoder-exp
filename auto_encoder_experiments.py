# -*- coding: utf-8 -*-
"""Auto-Encoder Experiments.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lnqJt_HGcE6RKrEZzHnc-SsaZk_qANnh
"""

# Commented out IPython magic to ensure Python compatibility.
import torch 
import numpy
from torch import nn
from torch.autograd import Variable
import numpy as np
from tqdm import tqdm
# %matplotlib inline

import os
import matplotlib.pyplot as plt

# It seems like training loss drops with jacobian norm?
# need some level of seperation
# residual connection maybe able to help bound operator norm of the jacobian
# Relu not working great

# snippets:
# x_list = [torch.FloatTensor(1, input_dim).uniform_(-1, 1) for i in range(nb_fixed_point)]
# x_list = [torch.FloatTensor(nb_fixed_point, input_dim).uniform_(-1, 1)]
# x_list_unormalized = [torch.Tensor(np.random.uniform(-np.sqrt(input_dim), np.sqrt(input_dim), size = (1, input_dim))) for i in range(nb_fixed_point)]

# activation function: softplus or cos, sigmoid. I guess 1-lipschitz and exepctation < 1

class Multivariate_Jacobian_Network(nn.Module):
  def __init__(self, input_dim, hidden_dim, nb_layer, host, device):
    super(Multivariate_Jacobian_Network, self).__init__()
    self.input_dim = input_dim
    self.hidden_dim = hidden_dim
    self.nb_layer = nb_layer
    self.w_list = nn.ParameterList([])
    self.b_list = nn.ParameterList([])
    if self.nb_layer > 2:
      self.scaing_factor = np.sqrt(self.hidden_dim)
    else:
      self.scaing_factor = np.sqrt(self.input_dim)
    self.w_in = torch.nn.Parameter(torch.randn(input_dim, self.hidden_dim)  , requires_grad = True) 
    self.b_in = torch.nn.Parameter(torch.randn(1, self.hidden_dim)  , requires_grad = False)  
    if self.nb_layer > 2:
      for _ in range(self.nb_layer - 2):
        prev_hidden_dim = self.hidden_dim 
        # self.hidden_dim = int(self.hidden_dim / 2) 
        self.w_list.append(torch.nn.Parameter(torch.randn(prev_hidden_dim, self.hidden_dim))) 
        self.b_list.append(torch.nn.Parameter(torch.randn(1, self.hidden_dim), requires_grad = False)) 
    self.w_out = torch.nn.Parameter(torch.randn(self.hidden_dim, input_dim) , requires_grad = True)
    self.b_out = torch.nn.Parameter(torch.randn(1, input_dim) , requires_grad = False)

    self.host = host
    self.device = device


  def act(self, x):
    # return torch.sin(x) + x/5
    # return torch.cos(x) #- x
    # return torch.sin(x)
    # return torch.tanh(x)
    # return torch.nn.functional.relu(x) #*0.5 #+ np.log(2)
    return torch.sigmoid(x)
    # return torch.tanh(x)
    # return x
    # return torch.nn.functional.softplus(x)

  def forward(self, x):
    mini_batch_size = x.shape[0]
    x_w_grad = Variable(x.clone(), requires_grad = True)
    
    normalize_factor = (np.sqrt(1/0.3)) # sigmoid
    beta = 1.0
    # normalize_factor = (np.sqrt(1/0.5))
    # normalize_factor = 1
    hid = self.act(x_w_grad @ self.w_in / np.sqrt(self.input_dim)* normalize_factor + self.b_in / np.sqrt(self.input_dim)* normalize_factor) 

    # hid = self.act(x_w_grad @ self.w_in / np.sqrt(self.input_dim) + self.b_in * beta) 
    
    for i,w in enumerate(self.w_list):
      # hid = self.act(hid@ w / np.sqrt(self.hidden_dim) + self.b_list[i] * beta) 
      hid = self.act(hid@ w / np.sqrt(self.hidden_dim) * normalize_factor + self.b_list[i] / np.sqrt(self.hidden_dim)* normalize_factor ) 
                                                        
    y = (hid @ self.w_out / np.sqrt(self.hidden_dim) * normalize_factor  + self.b_out / np.sqrt(self.hidden_dim)* normalize_factor) 
    
    # y = (hid @ self.w_out / np.sqrt(self.hidden_dim) + self.b_out * beta) 
    
    jac, norm_loss = self.compute_jacobian(y, x_w_grad)
    
    return y, jac
  
  def compute_jacobian(self, y, x):
    norm_loss = 0
    mini_batch_size = x.shape[0]
    
    jacobian = torch.zeros(self.input_dim, mini_batch_size, self.input_dim, device=self.device)
    grad_one_hot = torch.zeros(mini_batch_size, self.input_dim, device=self.device)
    
    for i in range(self.input_dim):
      grad_one_hot.zero_()
      grad_one_hot[:,i] = 1
      jacobian[i] = torch.autograd.grad(y, x, grad_one_hot, create_graph=True)[0]
      a = torch.autograd.grad(y, x, grad_one_hot, create_graph=True)[0]
      
      norm_loss = torch.sum(a * a)
      
    return torch.transpose(jacobian, dim0 = 0, dim1 = 1), norm_loss

def test(input_dim, hidden_dim, nb_layer, x_list, T, learning_rate = 1):
  host = torch.device("cpu")
  device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")

  model = Multivariate_Jacobian_Network(input_dim, hidden_dim, nb_layer, host, device).to(device)
        
  criterion = torch.nn.MSELoss(reduction='mean')
  # optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
  optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)

  nb_fixed_points = len(x_list)
  x_batch_tensor = torch.cat(x_list, axis=0).to(device)

  jac_norms = np.zeros((nb_fixed_points, T))
  losses = np.zeros((nb_fixed_points, T))
  
  if nb_layer > 2:
    w_compare_last_save = model.w_list[0].clone().to(host).data.numpy()
  else:
    w_compare_last_save = model.w_out.clone().to(host).data.numpy()

  for t in range(T):
    # clear loss
    all_loss = 0 

    # get predictions and jacobians
    y_pred, jac = model(x_batch_tensor)
    np_jac = jac.to(host).data.numpy()

    # construct loss
    rec_loss = criterion(y_pred, x_batch_tensor) 
    loss = rec_loss

    # optimize
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # add loss
    all_loss += rec_loss.item()

    # record losses and jacobian norm at each iteration
    for x_id, x in enumerate(x_list):
      jac_norms[x_id, t] = np.linalg.norm(np_jac[x_id,:,:], ord=2)
      loss_diss_vec = x.to(host).data.numpy() - y_pred[x_id,:].to(host).data.numpy()
      losses[x_id, t] = np.linalg.norm(loss_diss_vec)
    # if t%1000 == 0:  
    #   print(t, all_loss)
    #   print(t, jac_norms[0, t])
  # print(t, all_loss)
  if nb_layer > 2:
    w_new = model.w_list[0].clone().to(host).data.numpy()
  else:
    w_new = model.w_out.clone().to(host).data.numpy()
  
  # for x_id, x in enumerate(x_list):
  #   x_pretubed = x + 0.1
  #   # print('x_id: ' + str(x_id))
  #   # x_pretubed = torch.Tensor(np.random.normal(size=x.shape))
  #   diff = x.data.numpy() - x_pretubed.data.numpy()
  #   print(np.linalg.norm(diff, ord="fro"))
  #   for _ in range(10):
  #     x_pretubed, _, = model(x_pretubed)
  #     diff = x.data.numpy() - x_pretubed.data.numpy()
  #     print(np.linalg.norm(diff, ord="fro"))

  dirname = str("nb_") + str(nb_fixed_points)
  if not os.path.exists(dirname):
    os.makedirs(dirname)
  name = str("inputDim_") + str(input_dim) + "_" + str("hiddenDim_") + str(hidden_dim) + "_"  + str("nbLayer_") + str(nb_layer) + "_" + str("lr_") + str(learning_rate) + ".npy"
  np.save(dirname + "/" + name, jac_norms)

  for i in range(nb_fixed_points):
    pass
    # first = jac_norms[i,0]
    # last = jac_norms[i,-1]
    # print(input_dim, hidden_dim, nb_layer, first, last, ((last - first).mean()), ((last - first).mean())/first.mean(), np.linalg.norm(w_new - w_compare_last_save, ord=2) / model.scaing_factor)

    # plt.figure()
    # plt.plot(jac_norms[i,:], label = 'jacobian')
    # plt.plot(losses[i,:] / (1e-13 +np.max(losses[i,:])) * np.max(jac_norms[i,:]) , label = 'rescaled training loss')
    # plt.legend()
    # plt.title(str(all_loss))
    # name = str(input_dim) + "_" + str(hidden_dim) + "_"  + str(nb_layer) + "_" + str(learning_rate) + "_" + str(i) + ".png"
    # plt.savefig(name)
  
  return jac_norms[:,0], jac_norms[:,-1], np.linalg.norm(w_new - w_compare_last_save, ord=2) / model.scaing_factor, all_loss

T = 1000
M = 100

# if len(x_list) > 1:
#   diff = x_list[0].data.numpy() - x_list[1].data.numpy()
#   # print(x_list[0].data.numpy())
#   # print(x_list[1].data.numpy())
#   print("difference: " + str(np.linalg.norm(diff, ord="fro")))

# generate the data first    
for input_dim in ([16]):
  for nb_fixed_points in ([1, 2]):
    all_x_list = []
    for i in range(M):
      # generate input data
        x_list = []
        for _ in range(nb_fixed_points):
          a = np.random.uniform(-np.sqrt(input_dim), np.sqrt(input_dim), size = (1, input_dim))
          a_normalized = a /np.linalg.norm(a)
          x_list.append(torch.Tensor(a_normalized))
        all_x_list.append(x_list)

    for hidden_dim in ([100, 1000, 10000]):
      for nb_layer in [2]:
        for lr in ([1]):

          first = np.zeros((M, nb_fixed_points))
          last = np.zeros((M, nb_fixed_points))
          
          weight_norms = np.zeros(M)
          losses = np.zeros(M)

          for i in tqdm(range(M)):
            x_list = all_x_list[i]
            first[i,:], last[i,:], weight_norms[i], losses[i] = test(input_dim, hidden_dim, nb_layer, x_list, T, learning_rate=lr)

          print(nb_layer, nb_fixed_points, input_dim, hidden_dim, losses.mean(), weight_norms.mean(), first.mean(), last.mean(), ((last - first).mean()), ((last - first).mean())/first.mean())
          print(first.std(), last.std(), (last - first).std())